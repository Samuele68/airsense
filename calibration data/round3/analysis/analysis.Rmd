---
title: "Analysis of long term data"
output: html_notebook
---

Analysis of the data sets using linear regression.
See [this very nice post for an introduction with R](https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/).

Let's first open the round 1 data.

Collection of data was started on 18 / 5 / 2017
0 ms at sensor
16:49 at mobile phone
15:53 at monitor time (+/- 2 minutes !)


```{r}
round1_raw <- read.csv(file="../../round1/data/AIRSENSE.CSV", header=TRUE, sep=" ", stringsAsFactors = FALSE)

start_TS <- as.POSIXct('2017-05-18 15:53:00')
round1_raw$timestamp <- start_TS + floor(round1_raw$timestamp / 1000)

# a bit of renaming
names(round1_raw)[names(round1_raw) == "rawNO2_Mics"] <- "custom_MICS"

round1_ref <- read.csv(file="../../round1/data/reference.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
names(round1_ref)[names(round1_ref) == "TimeStamp"] <- "timestamp"
names(round1_ref)[names(round1_ref) == "NO2CNC1.AVG.PPB."] <- "NO2"
names(round1_ref)[names(round1_ref) == "NOXCNC1.AVG.PPB."] <- "NOX"
names(round1_ref)[names(round1_ref) == "NOCNC1.AVG.PPB."] <- "NO"

round1_ref$timestamp <- as.POSIXct(round1_ref$timestamp, format='%m/%d/%Y %H:%M:%S')

rm(start_TS)
```

Now we need to make the timestamps match, we will cut out periods of time where there is missing data and interpolate all the points from our sensor to match the reference.

```{r}
interpolateColumn <- function (raw, ref, colname) {
  ip <- approx(raw$timestamp,raw[[colname]], xout = ref$timestamp, rule = 2, method = "linear", ties = mean)
  temp <- data.frame(timestamp = ip$x, col = ip$y)
  names(temp)[names(temp) == 'col'] <- colname
  return(temp)
}

cleanAndMatch <- function(raw, ref) {
  # -1 are actually failures, remove them
  newraw <- raw
  newref <- ref
  newraw$humidity <- ifelse(newraw$humidity == -1, NA, newraw$humidity)
  newraw$temperature <- ifelse(newraw$temperature == -1, NA, newraw$temperature)
  
  # remove data outside the time span
  first_useful_ts <- max(newraw$timestamp[1], newref$timestamp[1])
  last_useful_ts <- min(newraw$timestamp[nrow(newraw)], newref$timestamp[nrow(newref)])
  
  newraw <- newraw[which(newraw$timestamp <= last_useful_ts & newraw$timestamp >= first_useful_ts ), ]
  newref <- newref[which(newref$timestamp <= last_useful_ts & newref$timestamp >= first_useful_ts ), ]
  
  # remove holes of more than 5 minutes:
  for (i in 2:nrow(newref)) {
    if (difftime(newref[i, 'timestamp'], newref[i-1, 'timestamp'], units=c('mins')) > 5 ) {
      print(paste('hole found in newref data: start', newref[i-1, 'timestamp'], 'end', newref[i, 'timestamp']))
      newraw <- newraw[which(! (newraw$timestamp > newref[i-1, 'timestamp'] & newraw$timestamp < newref[i, 'timestamp'])), ]
    }
  }
  
  for (i in 2:nrow(newraw)) {
    if (difftime(newraw[i, 'timestamp'], newraw[i-1, 'timestamp'], units=c('mins')) > 5 ) {
      print(paste('hole found in newraw data: start', newraw[i-1, 'timestamp'], 'end', newraw[i, 'timestamp']))
      newref <- newref[which(! (newref$timestamp > newraw[i-1, 'timestamp'] & newref$timestamp < newraw[i, 'timestamp'])), ]
    }
  }
  
  # all data between 00:45 and 1:05 is noise and has to be removed from analysis
  newraw <- newraw[ - which((as.numeric(format(newraw$timestamp, "%H")) == 0 &
                               as.numeric(format(newraw$timestamp, "%M")) >=45) |
                              (as.numeric(format(newraw$timestamp, "%H")) == 1 &
                                 as.numeric(format(newraw$timestamp, "%M")) <=5)), ]
  
  newref <- newref[ - which((as.numeric(format(newref$timestamp, "%H")) == 0 &
                               as.numeric(format(newref$timestamp, "%M")) >=45) |
                              (as.numeric(format(newref$timestamp, "%H")) == 1 &
                                 as.numeric(format(newref$timestamp, "%M")) <=5)), ]
  
  # now the let's match the two data sets, we'll interpolate the newraw data to match the newreference
  temp_newraw <- data.frame()
  
  for (name in names(newraw)) {
    if (name != 'timestamp') {
      if (nrow(temp_newraw) == 0) {
        temp_newraw = interpolateColumn(newraw, newref, name)
      } else {
        t = interpolateColumn(newraw, newref, name)
        temp_newraw[[name]] = t[[name]]
      }
    }
  }
  list(raw = temp_newraw, ref = newref)
}

t = cleanAndMatch(round1_raw, round1_ref)
round1_raw_ip = t$raw
round1_ref_ip = t$ref
rm(t)
```

Instead of using all the samples, we can use 30 minutes averages and put everything into one big data frame:

```{r}
subsample <- function(data, interval) {
  temp = split(data, cut(strptime(data$timestamp, format="%F %R"), interval))
  
  subsampled = data.frame(timestamp = names(temp))
  for (name in names(data)) {
    if (name != 'timestamp') {
      avgs = sapply(temp, function(x)mean(x[[name]]))
      subsampled[[name]] = avgs
    }
  }
  subsampled
}

collate <- function(raw, ref, interval) {
  raw_ss <- subsample(raw, interval)
  raw_ss$timestamp <- as.POSIXct(raw_ss$timestamp)
  ref_ss <- subsample(ref, interval)
  ref_ss$timestamp <- as.POSIXct(ref_ss$timestamp)
  all <- raw_ss
  all$refNO2 = ref_ss$NO2
  all
}

round1 <- collate(round1_raw_ip, round1_ref_ip, '30 mins')
head(round1)
```

Let's extend the data with new inputs (will see about it later):

```{r}
extendInput <- function(all) {
  all$temperature_sq = all$temperature ^ 2
  all$temperature_cu = all$temperature ^ 3
  
  all$humidity_sq = all$humidity ^ 2
  all$humidity_cu = all$humidity ^ 3
  
  all$custom_MICSxT <- all$custom_MICS * all$temperature
  all$custom_MICSxH <- all$custom_MICS * all$humidity
  
  all$custom_MICS_sq = all$custom_MICS ^ 2
  all$custom_MICS_cu = all$custom_MICS ^ 3
  all
}

round1 <- extendInput(round1)
head(round1)
```

Let's do the linear regression using mics, temperature and humidity and their 2nd power too.
I hypothesise that custom_MICS can interact with temperature and humidity but not their powers.
I also hypothesise that temperature and humidity do not contribute to the estimation alone, but only as coefficients of the NO2 sensor.

```{r}
round1_model_thm <- lm(refNO2 ~
                          custom_MICS + custom_MICS_sq +
                          custom_MICSxT + custom_MICSxH
                        , data=round1)
summary(round1_model_thm)

rmse = function(m, o){
  sqrt(mean((m - o)^2))
}

benchmark <- function(input, model) {
  errs <- data.frame(timestamp = input$timestamp, reference = input$refNO2, fitted = predict(model, input))
  # things can go strage, remove some extreme spikes
  errs <- errs[which(abs(errs$fitted) < 100), ]

  print(paste('mean absolute error', mean(abs(errs$reference - errs$fitted)), 'ppb'))
  print(paste('mean rmse', mean(rmse(errs$reference, errs$fitted)), 'ppb'))

  xrange <- range(errs$timestamp)

  plot(errs$timestamp, errs$reference, type='l', xlab='time', ylab='ppb', col = 'red')
  axis.POSIXct(1, at=seq(xrange[1], xrange[2], by="day"), format="%b %d")
  lines(errs$timestamp, errs$fitted, type='l', lty = 2, col = 'blue')
  legend("topright", legend = c("reference", "fitted"),
         text.width = strwidth("reference"),
         col=c('red', 'blue'),
         lty = 1:2, xjust = 1, yjust = 1)
  
  rm(xrange)
}

benchmark(round1, round1_model_thm)
```

The RMSE shows that the mesurement has a +- 5 ppb error which is not great.


## Round 2

Now let's basically repeat the same for round 2:

- Started: 10/01/2018
11:22:39 at mobile phone
11:23:21 at reference
0 at sensor

Let's load and parse the data:

```{r}
round2_raw <- read.csv(file="../../round2/data/AIRSENSE.CSV", header=TRUE, sep=",", stringsAsFactors = FALSE)

start_TS <- as.POSIXct('2018-01-10 11:23:21')

round2_raw$timestamp <- start_TS + floor(round2_raw$timestamp / 1000)


# remove data that was not gathered
round2_raw <- round2_raw[ , -which(names(round2_raw) %in% c('ratio_NH3','ratio_CO','ratio_NO2', 'nh3', 'co', 'no2', 'c3h8', 'c4h10', 'ch4', 'h2', 'c2h5oh'))]

round2_ref <- read.csv(file="../../round2/data/reference.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
round2_ref$timestamp <- as.POSIXct(paste(round2_ref$Date, round2_ref$Time))
round2_ref <- round2_ref[ , -which(names(round2_ref) %in% c('Date','Time'))]
names(round2_ref)[names(round2_ref) == "NO2CNC1"] <- "NO2"
names(round2_ref)[names(round2_ref) == "NOXCNC1"] <- "NOX"
names(round2_ref)[names(round2_ref) == "NOCNC1"] <- "NO"

rm(start_TS)
```

Now we need to match the inputs, collate and extend:

```{r}
t = cleanAndMatch(round2_raw, round2_ref)
round2_raw_ip = t$raw
round2_ref_ip = t$ref
rm(t)

round2 <- collate(round2_raw_ip, round2_ref_ip, '30 mins')
head(round2)
```

Here there are some "holes" in the data that will cause trouble later, we need to get rid of them:

```{r}
round2 = round2[which(!is.na(round2$custom_MICS)), ]
round2 = round2[which(!is.na(round2$refNO2)), ]
```

Let's extend this further:

```{r}
extendInput2 <- function(all) {
  all <- extendInput(all)
  
  all$pressure_sq = all$pressure ^ 2
  all$pressure_cu = all$pressure ^ 3
  all$alphadiff_sq = all$alphadiff ^ 2
  all$alphadiff_cu = all$alphadiff ^ 3
  all$alphadiffxT <- all$alphadiff * all$temperature
  all$alphadiffxH <- all$alphadiff * all$humidity
  
  all$alphadiffxP <- all$alphadiff * all$pressure
  all$custom_MICSxP <- all$custom_MICS * all$pressure
  all
}

round2 <- extendInput2(round2)
head(round2)
```


What happens if we use the model from round 1 on the data from round 2?

```{r}
benchmark(round2, round1_model_thm)
```

Performances are quite crap.


Now let's create a model for round 2, we will create one with only temperature, humidity and MICS as done before:

```{r}
round2_model_thm <- lm(refNO2 ~
                          custom_MICS + custom_MICS_sq + custom_MICSxT + custom_MICSxH
                        , data=round2)
summary(round2_model_thm)

benchmark(round2, round2_model_thm)
```

It works similarly to what was happening in round 1, with the round 1 data.
Now let's also add pressure and the alphasense sensor diff.

I hypotehise that the alpha sensor has interaction with temp, hum and pressure like the MICS:

```{r}
round2_model_thmpa <- lm(refNO2 ~
                            custom_MICS + custom_MICS_sq +
                            custom_MICSxT +
                            custom_MICSxH +
                            custom_MICSxP +
                            
                            alphadiff + alphadiff_sq +
                            alphadiffxT +
                            alphadiffxH +
                            alphadiffxP
                          , data=round2)
summary(round2_model_thmpa)

benchmark(round2, round2_model_thmpa)
```

Now, this looks much, much better.

Let's notice one thing: pressure has quite low effect on the sensors, maybe it's worth actually removing it completely:

```{r}
round2_model_thma <- lm(refNO2 ~
                            custom_MICS + custom_MICS_sq +
                            custom_MICSxT +
                            custom_MICSxH +

                            alphadiff + alphadiff_sq +
                            alphadiffxT +
                            alphadiffxH
                          , data=round2)
summary(round2_model_thma)

benchmark(round2, round2_model_thma)

```
The accuracy is a bit worse, but let's keep this model for comparison.


## Round 3

Here the datasets are a bit different, let's load them:

Started on 2/01/2019 at 13:45:48
Time at the monitoring station: 13:47:29

```{r}
round3_raw <- read.csv(file="../data/airsense.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

round3_raw$timestamp <- as.POSIXct(sub('\\..*', '', round3_raw$timestamp), format='%Y-%m-%dT%H:%M:%S')

# to match time on the station, we need to move all timestamps by a time difference:

td <- difftime(as.POSIXct('2/01/2019 13:45:48', format='%d/%m/%Y %H:%M:%S'), as.POSIXct('2/01/2019 13:47:29', format='%d/%m/%Y %H:%M:%S'), units = c('secs'))

round3_raw$timestamp <- round3_raw$timestamp - td

round2_raw$humidity <- ifelse(round2_raw$humidity == -1, NA, round2_raw$humidity)
round2_raw$temperature <- ifelse(round2_raw$temperature == -1, NA, round2_raw$temperature)
```

Reference data is split among different files:

```{r}
#round3_ref <- read.csv(file="../data/reference_2019-01-04_2019-01-17.csv", header=TRUE, sep=";", stringsAsFactors = FALSE)
#round3_ref$timestamp <- as.POSIXct(round3_ref$datetime, format='%d-%m-%y %H:%M')
#round3_ref$NOX <- rep(NA, nrow(round3_ref))
#round3_ref$NO <- rep(NA, nrow(round3_ref))
#round3_ref <- round3_ref[ , -which(names(round3_ref) %in% c('datetime'))]

round3_ref <- data.frame()

readLogCSV <- function(filename) {
  temp <- read.csv(file=filename, header=TRUE, sep=",", stringsAsFactors = FALSE)
  names(temp)[names(temp) == "NO2CNC1"] <- "NO2"
  names(temp)[names(temp) == "NOXCNC1"] <- "NOX"
  names(temp)[names(temp) == "NOCNC1"] <- "NO"

  temp$timestamp <- as.POSIXct(paste(temp$Date, temp$Time), format='%Y/%m/%d %H:%M')
  temp <- temp[ , -which(names(temp) %in% c('Date', 'Time'))]  
  temp
}

round3_ref <- readLogCSV("../data/Log-20190118-102419.log.csv")
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190122-093909.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190129-114056.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190201-110630.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190204-140411.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190208-150109.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190211-102551.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190214-104139.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190219-084616.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190301-115140.log.csv"))
round3_ref <- rbind(round3_ref, readLogCSV("../data/Log-20190311-114102.log.csv"))
```

Cleanup and put everything together:

```{r}
t = cleanAndMatch(round3_raw, round3_ref)
round3_raw_ip = t$raw
round3_ref_ip = t$ref
rm(t)

round3 <- collate(round3_raw_ip, round3_ref_ip, '30 mins')
round3 <- extendInput(round3)

round3 = round3[which(!is.na(round3$custom_MICS)), ]
round3 = round3[which(!is.na(round3$refNO2)), ]

extendInput3 <- function(all) {
  all <- extendInput2(all)
  all$cjmcu_red_sq = all$cjmcu_red ^ 2
  all$cjmcu_red_cu = all$cjmcu_red ^ 3
  all$cjmcu_ox_sq = all$cjmcu_ox ^ 2
  all$cjmcu_ox_cu = all$cjmcu_ox ^ 2
  all$cjmcu_redxT <- all$cjmcu_red * all$temperature
  all$cjmcu_redxH <- all$cjmcu_red * all$humidity
  all$cjmcu_redxP <- all$cjmcu_red * all$pressure
  all$cjmcu_oxxT <- all$cjmcu_ox * all$temperature
  all$cjmcu_oxxH <- all$cjmcu_ox * all$humidity
  all$cjmcu_oxxP <- all$cjmcu_ox * all$pressure
  all
}

round3 <- extendInput3(round3)
head(round3)
```


What happens if we use the model from round 1?

```{r}
benchmark(round3, round1_model_thm)
```

surprisingly, it doesn't look too bad !

Let's see if use the first model of round 2:

```{r}
benchmark(round3, round2_model_thm)
```

Doesn't pick the spikes very well, but the overall trend is OK.


Now with the second and more precise model:

```{r}
benchmark(round3, round2_model_thmpa)
```

This is completely out of range. Why is that? Maybe the pressure?

If we remove the pressure?

```{r}
benchmark(round3, round2_model_thma)
```
Even worse.
So it looks like the alphadiff channel is too overfitted over the 2nd round and here doesn't work at all.


Let's build the same models using round 3 data, but only using the first week of data:

```{r}
round3_week1 <- round3[which( difftime(round3$timestamp, round3[1, 'timestamp'], units=c('days')) <7 ), ]
# this actually less than a week!

round3_model_thm <- lm(refNO2 ~
                          custom_MICS + custom_MICS_sq +
                          custom_MICSxT +
                          custom_MICSxH
                        , data=round3_week1)
summary(round3_model_thm)
benchmark(round3, round3_model_thm)


round3_model_thma <- lm(refNO2 ~
                            custom_MICS + custom_MICS_sq +
                            custom_MICSxT +
                            custom_MICSxH +

                            alphadiff + alphadiff_sq +
                            alphadiffxT+
                            alphadiffxH
                          , data=round3_week1)
summary(round3_model_thma)
benchmark(round3, round3_model_thma)

round3_model_thmpa <- lm(refNO2 ~
                            custom_MICS + custom_MICS_sq +
                            custom_MICSxT +
                            custom_MICSxH +
                            custom_MICSxP +
                            
                            alphadiff + alphadiff_sq +
                            alphadiffxT +
                            alphadiffxH +
                            alphadiffxP
                          , data=round3_week1)
summary(round3_model_thmpa)
benchmark(round3, round3_model_thmpa)
```

What's interesting in these charts is that it clearly shows the overfitting in the first week.
The THM does quite OK amd does not overfit, the THMA seems better at picking spikes, but has a bit lower overall accuracy than THM alone, the THMPA does very well in the first week, but then it looses accuracy a lot.

So it looks like the THM is the one that keeps the best accuracy over time.


Let's add also the new sensor, the CJMCU 4541.
First of all, are the MICS and the CJMCU different? Let's find it out:

```{r}
xrange <- range(round3_raw$timestamp)

plot(round3_raw$timestamp, round3_raw$custom_MICS, type='l', xlab='time', col = 'red')
axis.POSIXct(1, at=seq(xrange[1], xrange[2], by="day"), format="%b %d")
lines(round3_raw$timestamp, round3_raw$cjmcu_ox, type='l', lty = 2, col = 'blue')
legend("topright", legend = c("MICS", "CJMCU"),
       text.width = strwidth("CJMCU"),
       col=c('red', 'blue'),
       lty = 1:2, xjust = 1, yjust = 1)
```
They look almost identical except from a scaling factor. So we should expect almost identical results if we use the MICS or the CJMCU.

We train the model on the first week of the dataset only as before.
We treat the cjmcu variables as the custom_MICS, we include the alphasense but we exclude pressure.

```{r}
round3_model3_thmac <- lm(refNO2 ~
                             custom_MICS + custom_MICS_sq +
                             custom_MICSxT +
                             custom_MICSxH +

                             alphadiff + alphadiff_sq +
                             alphadiffxT +
                             alphadiffxH +

                             cjmcu_red + cjmcu_red_sq +
                             cjmcu_redxT +
                             cjmcu_redxH +

                             cjmcu_ox + cjmcu_ox_sq +
                             cjmcu_oxxT +
                             cjmcu_oxxH + 
                            
                            cjmcu_red * cjmcu_ox
                           , data=round3_week1)
summary(round3_model3_thmac)
benchmark(round3, round3_model3_thmac)
```

Mmm, too overfitted.

What about alphasense alone? Is it any good?

```{r}
round3_model_tha <- lm(refNO2 ~
                          alphadiff + alphadiff_sq +
                          alphadiffxT +
                          alphadiffxH
                        , data=round3_week1)
summary(round3_model_tha)
benchmark(round3, round3_model_tha)
```

Like in round 2, it does something, but the data is too overfitted in the first week.


What if we remove alphasense then?
Can we trust only the CJMCU 4541 ?

```{r}
round3_model_thc <- lm(refNO2 ~
                             cjmcu_red + cjmcu_red_sq +
                             cjmcu_redxT +
                             cjmcu_redxH +

                             cjmcu_ox + cjmcu_ox_sq +
                             cjmcu_oxxT +
                             cjmcu_oxxH +

                             cjmcu_red * cjmcu_ox
                           , data=round3_week1)
summary(round3_model_thc)
benchmark(round3, round3_model_thc)

round3_model_thc2 <- lm(refNO2 ~
                             cjmcu_ox + cjmcu_ox_sq +
                             cjmcu_oxxT +
                             cjmcu_oxxH
                           , data=round3_week1)
summary(round3_model_thc2)
benchmark(round3, round3_model_thc2)
```

So the one that includes the RED channel is overfitted, but the one with only the NOX channel does fine, like if we used only the custom_MICS.

Let's put back alphasense again:
```{r}
round3_model_thac2 <- lm(refNO2 ~
                          alphadiff + alphadiff_sq +
                          alphadiffxT +
                          alphadiffxH +
                          
                          cjmcu_ox + cjmcu_ox_sq +
                          cjmcu_oxxT +
                          cjmcu_oxxH
                        , data=round3_week1)
summary(round3_model_thac2)
benchmark(round3, round3_model_thac2)
```

The results are quite good, but worse than with the CJMCU NOX channel alone overall.


## Conclusions

If we include all sorts of sensors the fitting with the data can increase substantially, but it will be overfitted.
The good old MICS 2714 seems to be working consistently well (or bad depending on hwo you see it) and is probably the only one that does not overfit.

If we plot the error over time, to understand drift, using the model from the first round:

```{r}

abserrors <- data.frame(timestamp = round1$timestamp, abserror = abs(round1$refNO2 - predict(round1_model_thm, round1)))
abserrors <- rbind(abserrors, data.frame(timestamp = round2$timestamp, abserror = abs(round2$refNO2 - predict(round1_model_thm, round2))))
abserrors <- rbind(abserrors, data.frame(timestamp = round3$timestamp, abserror = abs(round3$refNO2 - predict(round1_model_thm, round3))))
abserrors <- abserrors[which(abserrors$abserror < 100), ]
abserrors$weeks <- floor(difftime(abserrors$timestamp, round1[1, 'timestamp'], units=c('week')))

boxplot(abserror~weeks, data=abserrors, main="Drift of Mics 2714 + temp + hum", 
        xlab="Weeks from calibration", ylab="Error (ppb)")
```

There seems to be some sort of drift, because the error seems to become more spread in the weeks 85 to 92 compared to week 0, but, because there are no other weeks to check in between, it's hard to say.

Let's compare the solution with Mics and Alphasense, but only on round 3:

```{r}

abserrors <- data.frame(timestamp = round3$timestamp, abserror = abs(round3$refNO2 - predict(round3_model_thma, round3)))
abserrors <- abserrors[which(abserrors$abserror < 100), ]
abserrors$weeks <- floor(difftime(abserrors$timestamp, round3[1, 'timestamp'], units=c('week')))

boxplot(abserror~weeks, data=abserrors, main="Drift of Mics 2714 + temp + hum + Alphasense", 
   xlab="Weeks from calibration", ylab="Error (ppb)")
```

The situation is similar, with overall higher errors though especially considering that the model is much more close in time than the one shown above.


Other conclusions we can derive are:

- pressure doesn't add anything useful and should probably be excluded
- the RED channel of the CJMCU should be ignored
- the Alphasense sensor can lead to excellent results when used on data it was trained on, but when used on data where it was not trained on, it doesn't seem to add much useful information


